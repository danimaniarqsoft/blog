<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Landscape of Natural Language Processing</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Digital Frontier Blues -->
    <!-- Application Structure Plan: The infographic is designed as a top-to-bottom narrative scroll. It starts with a high-level introduction, moves to the historical evolution to provide context, then dives into a comparative analysis of the core NLP approaches using a radar chart for a quick visual summary. It then explains the practical steps of the NLP pipeline with a CSS-based flowchart, and finally compares key representation techniques. This linear, story-driven structure was chosen to guide the user logically from "what is it?" to "how does it work?", making the dense report material accessible to a broader audience. -->
    <!-- Visualization & Content Choices: 
        - Report Info: Historical progression of NLP. Goal: Change. Viz/Presentation: HTML/CSS Timeline. Justification: A vertical timeline is the most intuitive way to show chronological evolution and is built without SVG/Mermaid, adhering to constraints. Library/Method: HTML/CSS with Tailwind.
        - Report Info: Trade-offs between Rule-Based, ML, and DL approaches. Goal: Compare. Viz/Presentation: Radar Chart. Justification: A radar chart effectively compares multiple subjects (the 3 approaches) across multiple quantitative axes (Performance, Scalability, Interpretability, etc.), providing an immediate, holistic comparison. Library/Method: Chart.js (Canvas).
        - Report Info: Text preprocessing steps. Goal: Organize/Process Flow. Viz/Presentation: HTML/CSS Flowchart. Justification: A clean, card-based flowchart clearly illustrates the sequence of the NLP pipeline. Using Unicode arrows and Tailwind styling is a lightweight and compliant method. Library/Method: HTML/CSS with Tailwind.
        - Report Info: TF-IDF vs. Word Embeddings. Goal: Compare/Inform. Viz/Presentation: Side-by-side comparison cards. Justification: For this conceptual comparison, clear textual explanations in styled cards are most effective. Library/Method: HTML/CSS with Tailwind.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F0F4F8;
            color: #1F2937;
        }
        .chart-container {
            position: relative;
            margin: auto;
            height: 50vh;
            max-height: 450px;
            width: 100%;
            max-width: 500px;
        }
        .timeline-item::before {
            content: '';
            position: absolute;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background-color: white;
            border: 4px solid #00C49F;
            top: 50%;
            left: -10px;
            transform: translateY(-50%);
            z-index: 1;
        }
    </style>
</head>
<body class="antialiased">

    <!-- Header Section -->
    <header class="bg-[#0A2A4D] text-white text-center py-20 px-6">
        <h1 class="text-4xl md:text-6xl font-extrabold mb-4">The Landscape of Natural Language Processing</h1>
        <p class="max-w-3xl mx-auto text-lg md:text-xl text-gray-300">An infographic exploring how machines learn to understand, interpret, and generate human language, from foundational rules to the power of deep learning.</p>
    </header>

    <!-- Main Content -->
    <main class="container mx-auto px-6 py-16">

        <!-- Section 1: The Evolution of NLP -->
        <section class="mb-24">
            <h2 class="text-3xl md:text-4xl font-bold text-center mb-4">A Journey Through Time</h2>
            <p class="text-lg text-center max-w-2xl mx-auto text-gray-600 mb-16">NLP has transformed from manually crafted linguistic rules to sophisticated, self-learning neural networks. This evolution reflects a fundamental shift from human-encoded knowledge to data-driven discovery.</p>
            
            <div class="relative max-w-2xl mx-auto">
                <div class="absolute left-1/2 transform -translate-x-1/2 h-full w-1 bg-gray-300 rounded"></div>
                <!-- Timeline Item 1: Rule-Based -->
                <div class="relative mb-12">
                    <div class="md:flex items-center">
                        <div class="md:w-1/2"></div>
                        <div class="md:w-1/2 md:pl-12">
                            <div class="bg-white p-6 rounded-lg shadow-lg border-l-4 border-[#4D88FF]">
                                <span class="text-sm font-semibold text-[#4D88FF]">1950s - 1980s</span>
                                <h3 class="text-xl font-bold mt-1 mb-2">The Rule-Based Era</h3>
                                <p class="text-gray-600">Early systems relied on meticulously hand-crafted linguistic rules. They were precise for narrow domains but brittle and unable to scale or handle language's ambiguity.</p>
                            </div>
                        </div>
                    </div>
                    <div class="absolute left-1/2 transform -translate-x-1/2 top-1/2 -translate-y-1/2 w-5 h-5 rounded-full bg-white border-4 border-[#4D88FF]"></div>
                </div>
                <!-- Timeline Item 2: Statistical ML -->
                <div class="relative mb-12">
                    <div class="md:flex items-center">
                        <div class="md:w-1/2 md:pr-12 text-right">
                             <div class="bg-white p-6 rounded-lg shadow-lg border-r-4 border-[#00C49F]">
                                <span class="text-sm font-semibold text-[#00C49F]">1990s - 2010s</span>
                                <h3 class="text-xl font-bold mt-1 mb-2">The Statistical Revolution</h3>
                                <p class="text-gray-600">With more data and computing power, focus shifted to machine learning. Systems learned statistical patterns from text, making them more adaptable and robust than rule-based methods.</p>
                            </div>
                        </div>
                        <div class="md:w-1/2"></div>
                    </div>
                     <div class="absolute left-1/2 transform -translate-x-1/2 top-1/2 -translate-y-1/2 w-5 h-5 rounded-full bg-white border-4 border-[#00C49F]"></div>
                </div>
                <!-- Timeline Item 3: Deep Learning -->
                <div class="relative">
                    <div class="md:flex items-center">
                        <div class="md:w-1/2"></div>
                        <div class="md:w-1/2 md:pl-12">
                            <div class="bg-white p-6 rounded-lg shadow-lg border-l-4 border-[#0A2A4D]">
                                <span class="text-sm font-semibold text-[#0A2A4D]">2010s - Present</span>
                                <h3 class="text-xl font-bold mt-1 mb-2">The Deep Learning Wave</h3>
                                <p class="text-gray-600">Neural networks, especially Transformer models like BERT and GPT, revolutionized the field. These models learn complex language representations automatically, achieving state-of-the-art performance.</p>
                            </div>
                        </div>
                    </div>
                     <div class="absolute left-1/2 transform -translate-x-1/2 top-1/2 -translate-y-1/2 w-5 h-5 rounded-full bg-white border-4 border-[#0A2A4D]"></div>
                </div>
            </div>
        </section>

        <!-- Section 2: Core Approaches -->
        <section class="mb-24">
            <h2 class="text-3xl md:text-4xl font-bold text-center mb-4">Core Approaches: A Balancing Act</h2>
            <p class="text-lg text-center max-w-2xl mx-auto text-gray-600 mb-12">Modern NLP is dominated by three core approaches, each with a distinct trade-off between performance, scalability, and interpretability. The choice of approach depends heavily on the specific problem, available data, and required transparency.</p>
            
            <div class="grid md:grid-cols-2 lg:grid-cols-5 gap-8 items-center">
                <div class="lg:col-span-3">
                    <div class="bg-white p-6 rounded-lg shadow-lg">
                       <div class="chart-container">
                            <canvas id="approachComparisonChart"></canvas>
                        </div>
                    </div>
                </div>
                <div class="lg:col-span-2 space-y-4">
                    <div class="bg-white p-4 rounded-lg shadow-md">
                        <h4 class="font-bold text-lg text-[#4D88FF]">Rule-Based</h4>
                        <p class="text-sm text-gray-600">High interpretability, low scalability. Best for very specific, stable tasks.</p>
                    </div>
                    <div class="bg-white p-4 rounded-lg shadow-md">
                        <h4 class="font-bold text-lg text-[#00C49F]">Machine Learning</h4>
                        <p class="text-sm text-gray-600">Good balance of performance and scalability, but requires feature engineering.</p>
                    </div>
                    <div class="bg-white p-4 rounded-lg shadow-md">
                        <h4 class="font-bold text-lg text-[#0A2A4D]">Deep Learning</h4>
                        <p class="text-sm text-gray-600">Highest performance and scalability, but often a "black box" requiring vast data.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 3: The NLP Pipeline -->
        <section class="mb-24">
            <h2 class="text-3xl md:text-4xl font-bold text-center mb-4">The NLP Pipeline in Action</h2>
            <p class="text-lg text-center max-w-2xl mx-auto text-gray-600 mb-16">Before any model can learn, raw text must be cleaned and structured. This preprocessing pipeline transforms messy, unstructured data into a standardized format that algorithms can understand.</p>

            <div class="flex flex-col md:flex-row items-center justify-center space-y-8 md:space-y-0 md:space-x-8">
                <!-- Step 1 -->
                <div class="bg-white p-6 rounded-lg shadow-lg text-center max-w-xs">
                    <div class="text-4xl font-bold text-[#4D88FF] mb-2">1</div>
                    <h3 class="text-xl font-bold mb-2">Tokenization</h3>
                    <p class="text-gray-600">Breaking raw text into individual words or sub-words called tokens.</p>
                </div>
                <div class="text-4xl font-bold text-gray-400 hidden md:block">&#10140;</div>
                <div class="text-4xl font-bold text-gray-400 md:hidden">&#11163;</div>
                <!-- Step 2 -->
                 <div class="bg-white p-6 rounded-lg shadow-lg text-center max-w-xs">
                    <div class="text-4xl font-bold text-[#00C49F] mb-2">2</div>
                    <h3 class="text-xl font-bold mb-2">Normalization</h3>
                    <p class="text-gray-600">Cleaning text by converting to lowercase, removing stop words, and lemmatizing words to their root form.</p>
                </div>
                <div class="text-4xl font-bold text-gray-400 hidden md:block">&#10140;</div>
                <div class="text-4xl font-bold text-gray-400 md:hidden">&#11163;</div>
                <!-- Step 3 -->
                 <div class="bg-white p-6 rounded-lg shadow-lg text-center max-w-xs">
                    <div class="text-4xl font-bold text-[#0A2A4D] mb-2">3</div>
                    <h3 class="text-xl font-bold mb-2">Vectorization</h3>
                    <p class="text-gray-600">Converting the clean tokens into numerical representations (vectors) for the model to process.</p>
                </div>
            </div>
        </section>

        <!-- Section 4: Text Representation -->
        <section>
            <h2 class="text-3xl md:text-4xl font-bold text-center mb-4">Representing Words as Numbers</h2>
            <p class="text-lg text-center max-w-2xl mx-auto text-gray-600 mb-12">Vectorization is where words become math. The two main approaches represent a leap in capturing meaning: from simple word counts to understanding rich, contextual relationships.</p>

            <div class="grid md:grid-cols-2 gap-8 max-w-4xl mx-auto">
                <div class="bg-white p-8 rounded-lg shadow-lg">
                    <h3 class="text-2xl font-bold mb-3">TF-IDF: The Statistician</h3>
                    <p class="text-gray-600 mb-4">Stands for Term Frequency-Inverse Document Frequency. This technique represents words based on how frequently they appear in a document, while down-weighting words that are common across all documents. It's great for keyword extraction but understands no context.</p>
                    <div class="bg-gray-100 p-4 rounded-md">
                        <p class="font-mono text-sm text-gray-700">"The cat sat" &#10142; [0, 1, 0, 1, 1, 0, ...]</p>
                        <p class="text-xs text-gray-500 mt-2">Result: A sparse vector where most values are zero.</p>
                    </div>
                </div>
                <div class="bg-white p-8 rounded-lg shadow-lg">
                    <h3 class="text-2xl font-bold mb-3">Word Embeddings: The Linguist</h3>
                    <p class="text-gray-600 mb-4">Techniques like Word2Vec and BERT represent words as dense vectors in a multi-dimensional space. Words with similar meanings are placed close together, capturing semantic relationships and context. This allows models to understand nuance and analogy.</p>
                    <div class="bg-gray-100 p-4 rounded-md">
                        <p class="font-mono text-sm text-gray-700">"king" &#10142; [0.2, -0.4, 0.7, ...]</p>
                        <p class="text-xs text-gray-500 mt-2">Result: A dense vector capturing rich semantic meaning.</p>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <!-- Footer -->
    <footer class="bg-[#0A2A4D] text-white text-center py-8 mt-16">
        <p>Infographic created based on "A Comprehensive Taxonomy and Landscape Analysis of Natural Language Processing".</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const ctx = document.getElementById('approachComparisonChart').getContext('2d');
            const approachChart = new Chart(ctx, {
                type: 'radar',
                data: {
                    labels: ['Performance', 'Scalability', 'Data Requirement', 'Interpretability', 'Ease of Build'],
                    datasets: [{
                        label: 'Rule-Based',
                        data: [4, 3, 2, 10, 5],
                        fill: true,
                        backgroundColor: 'rgba(77, 136, 255, 0.2)',
                        borderColor: 'rgb(77, 136, 255)',
                        pointBackgroundColor: 'rgb(77, 136, 255)',
                        pointBorderColor: '#fff',
                        pointHoverBackgroundColor: '#fff',
                        pointHoverBorderColor: 'rgb(77, 136, 255)'
                    }, {
                        label: 'Machine Learning',
                        data: [7, 7, 6, 6, 6],
                        fill: true,
                        backgroundColor: 'rgba(0, 196, 159, 0.2)',
                        borderColor: 'rgb(0, 196, 159)',
                        pointBackgroundColor: 'rgb(0, 196, 159)',
                        pointBorderColor: '#fff',
                        pointHoverBackgroundColor: '#fff',
                        pointHoverBorderColor: 'rgb(0, 196, 159)'
                    }, {
                        label: 'Deep Learning',
                        data: [10, 9, 9, 2, 3],
                        fill: true,
                        backgroundColor: 'rgba(10, 42, 77, 0.2)',
                        borderColor: 'rgb(10, 42, 77)',
                        pointBackgroundColor: 'rgb(10, 42, 77)',
                        pointBorderColor: '#fff',
                        pointHoverBackgroundColor: '#fff',
                        pointHoverBorderColor: 'rgb(10, 42, 77)'
                    }]
                },
                options: {
                    maintainAspectRatio: false,
                    elements: {
                        line: {
                            borderWidth: 3
                        }
                    },
                    scales: {
                        r: {
                            angleLines: {
                                color: 'rgba(0, 0, 0, 0.1)'
                            },
                            grid: {
                                color: 'rgba(0, 0, 0, 0.1)'
                            },
                            pointLabels: {
                                font: {
                                    size: 14,
                                    weight: '500'
                                },
                                color: '#1F2937'
                            },
                            ticks: {
                                backdropColor: '#F0F4F8',
                                color: '#6b7280'
                            },
                            min: 0,
                            max: 10
                        }
                    },
                    plugins: {
                        tooltip: {
                            callbacks: {
                                title: function(tooltipItems) {
                                    const item = tooltipItems[0];
                                    let label = item.chart.data.labels[item.dataIndex];
                                    if (Array.isArray(label)) {
                                      return label.join(' ');
                                    } else {
                                      return label;
                                    }
                                }
                            }
                        },
                        legend: {
                            position: 'top',
                            labels: {
                                font: {
                                    size: 14
                                }
                            }
                        }
                    }
                }
            });
        });
    </script>
</body>
</html>
